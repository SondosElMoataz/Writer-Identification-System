{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\farah\\anaconda3\\lib\\site-packages (3.4.2.17)\nRequirement already satisfied: numpy>=1.14.5 in c:\\users\\farah\\anaconda3\\lib\\site-packages (from opencv-python) (1.19.5)\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import histogram\n",
    "from matplotlib.pyplot import bar\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import io ,filters,feature,transform\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        plt.axis('off')\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(image):\n",
    "     \n",
    "    # Remove salt and pepper noise    \n",
    "    # Remove noise \n",
    "    median = cv2.medianBlur(image,5)\n",
    "    blur = cv2.GaussianBlur(image,(5,5),0)\n",
    "   \n",
    "    greyImg = image\n",
    "    \n",
    "    # Otsu's Binarization\n",
    "    ret3,img = cv2.threshold(image,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "   \n",
    "    # Remove header and footer\n",
    "    length, width = img.shape\n",
    "    up, down, left, right = 0, length - 1, 0, width - 1\n",
    "\n",
    "    minWidthOfLines = width/2\n",
    "    yes,contours,no = cv2.findContours(img, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    offsetHeader = 20\n",
    "    for i in contours:\n",
    "        x, y, w, h = cv2.boundingRect(i)\n",
    "        if w < minWidthOfLines:\n",
    "            continue\n",
    "        if y < length // 2:\n",
    "            up = max(up, y + offsetHeader)\n",
    "        else:\n",
    "            down = min(down, y - offsetHeader)\n",
    "\n",
    "    offset = 30\n",
    "    left = left + offset\n",
    "    right = right -offset\n",
    "    noHeaderAndFooter = img[up:down + 1, left:right + 1]\n",
    "    noHeaderAndFooter = np.asarray(noHeaderAndFooter)\n",
    "    \n",
    "    noHeaderAndFooterGrey = greyImg[up:down + 1, left:right + 1]\n",
    "    noHeaderAndFooterGrey = np.asarray(noHeaderAndFooterGrey) \n",
    "   \n",
    "    # To crop the image\n",
    "    row, col = noHeaderAndFooter.shape\n",
    "    tolerance = 15\n",
    "\n",
    "    sumOfRows = np.sum(noHeaderAndFooter, axis = 1)\n",
    "    rowIndices = np.where(sumOfRows< (col-tolerance)*255)\n",
    "    up = np.min(rowIndices)\n",
    "    down = np.max(rowIndices)\n",
    "\n",
    "    sumOfColoumns = np.sum(noHeaderAndFooter, axis = 0)\n",
    "    colIndices = np.where(sumOfColoumns< (row-tolerance)*255)\n",
    "    left = np.min(colIndices)\n",
    "    right = np.max(colIndices)\n",
    "\n",
    "    binarized = noHeaderAndFooter[up:down + 1, left:right + 1]\n",
    "    binarized = np.asarray(binarized)\n",
    "    \n",
    "    greyscale = noHeaderAndFooterGrey[up:down + 1, left:right + 1]\n",
    "    greyscale = np.asarray(greyscale)\n",
    "   \n",
    "    # Segmentation of Lines\n",
    "    rowIndicesShifted = np.roll(rowIndices, -1)\n",
    "    rowIndicesShifted = rowIndicesShifted[0]\n",
    "\n",
    "    transitionIndices = np.where(np.abs(rowIndices - rowIndicesShifted) > 10)\n",
    "    transitionIndices = transitionIndices[1]\n",
    "\n",
    "    rowIndices = rowIndices[0]\n",
    "\n",
    "    downIndices= rowIndices[transitionIndices]\n",
    "\n",
    "    transitionIndicesUp = np.insert(transitionIndices,0,-1)\n",
    "    transitionIndicesUp = np.delete(transitionIndicesUp,-1)\n",
    "\n",
    "    upIndices= rowIndices[transitionIndicesUp+1]\n",
    "    \n",
    "    segmentsBinarized = []\n",
    "    segmentsGrey = []\n",
    "    totalSize=0\n",
    "    whiteSpaceTolerance=0.97\n",
    "    for i in range(transitionIndices.shape[0]):\n",
    "        currSegment= noHeaderAndFooterGrey[upIndices[i]:downIndices[i] + 1, left:right + 1]\n",
    "        if((np.sum(currSegment))<(currSegment.shape[0]*currSegment.shape[1]*whiteSpaceTolerance*255)):\n",
    "            segmentsBinarized.append(noHeaderAndFooter[upIndices[i]:downIndices[i] + 1, left:right + 1])\n",
    "            segmentsGrey.append(currSegment)\n",
    "          \n",
    "    segmentsGrey=np.asarray(segmentsGrey)\n",
    "    \n",
    "#     show_images(segmentsGrey)\n",
    "    \n",
    "    return greyscale, binarized, segmentsBinarized, segmentsGrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showHist(imgHist):\n",
    "    bar(imgHist[1].astype(np.uint8), imgHist[0], width=0.8, align='center')\n",
    "    return imgHist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Feature Extraction LBP\n",
    "def LBP(greyscale):\n",
    "    lbp = local_binary_pattern(greyscale, 8, 3, method='uniform')\n",
    "    n_bins =10 #256 \n",
    "    imgHist = histogram(lbp, n_bins)\n",
    "    \n",
    "    return imgHist[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slant_angle(img):\n",
    "    edges = feature.canny(img, sigma=0.6)\n",
    "    hspace, angles, distances=transform.hough_line(edges)\n",
    "    accum, angles, dists = transform.hough_line_peaks(hspace, angles, distances)\n",
    "    angle = np.rad2deg(np.median(angles))\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew(img):\n",
    "    thresh=img\n",
    "    edges = cv2.Canny(thresh,50,200,apertureSize = 3)\n",
    "\n",
    "#     show_images([edges],['EDGEEEEEEEEEEEEEEEEEEEEEEEEE'])\n",
    "    lines = cv2.HoughLines(edges,1,np.pi/1000, 55)\n",
    "\n",
    "    d1 = OrderedDict()\n",
    "    if lines is None:\n",
    "        return 0\n",
    "    for i in range(len(lines)):\n",
    "        for rho,theta in lines[i]:\n",
    "            deg = np.rad2deg(theta)\n",
    "            if deg in d1:\n",
    "                d1[deg] += 1\n",
    "            else:\n",
    "                d1[deg] = 1\n",
    "                   \n",
    "    t1 = OrderedDict(sorted(d1.items(), key=lambda x:x[1] , reverse=False))\n",
    "#     print(list(t1.keys())[0],'Angle' ,thresh.shape)\n",
    "    angle =list(t1.keys())[0]\n",
    "    \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(greyscale1, binarized1, segmentsBinarized1, segmentsGrey1):\n",
    "    features=[]\n",
    "    features.extend(LBP(segmentsGrey1))\n",
    "#     features.append(get_slant_angle(segmentsGrey1))\n",
    "  #  features.append(deskew(segmentsGrey1))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(folder):\n",
    "    trainTime=0\n",
    "    feature_vector_all=[]\n",
    "    direcs = glob.glob (folder+\"/*\")\n",
    "    for direc in direcs:\n",
    "        if os.path.isdir(direc):\n",
    "#             print(direc)\n",
    "            files = glob.glob (direc+'/*')\n",
    "            for file in files:\n",
    "                #reading an imag\n",
    "                print(\"-----------------------\",file,\"------------------------------\")\n",
    "                img = cv2.imread(file,0) \n",
    "                tempStart =time.time()\n",
    "\n",
    "                #converting it to a gray image\n",
    "                #binarization process\n",
    "                \n",
    "                greyscale1, binarized1, segmentsBinarized1, segmentsGrey1 = Preprocess(img)\n",
    "#                 print(segmentsGrey1.shape)\n",
    "                for i in range(segmentsGrey1.shape[0]):\n",
    "                    feature_vector=[]\n",
    "     \n",
    "                    z = direc.split('\\\\')\n",
    "#                     print(z[1])\n",
    "                    y= int(z[1])\n",
    "                    if(y< 100):\n",
    "                        feature_vector.append(int(direc[8]))\n",
    "#                         print(int(direc[8]))\n",
    "                    if(y>100 and y<1000):\n",
    "                        feature_vector.append(int(direc[9]))\n",
    "#                         print(int(direc[9]))\n",
    "                    if(y>1000):\n",
    "                        feature_vector.append(int(direc[10]))\n",
    "#                         print(int(direc[10])) \n",
    "                    \n",
    "#                     feature_vector.append(int(direc[8]))\n",
    "                    \n",
    "                    features =extract_features(greyscale1, binarized1, segmentsBinarized1, segmentsGrey1[i])\n",
    "                    feature_vector.extend(features)\n",
    "                    feature_vector_all.append(feature_vector)\n",
    "                    \n",
    "                tempEnd =time.time()\n",
    "                trainTime+= tempEnd-tempStart \n",
    "\n",
    "    return feature_vector_all,trainTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "\n",
    "    distance =np.linalg.norm(x1-x2)\n",
    "    return distance\n",
    "\n",
    "# def KNN(test_point, training_features, labels, k): \n",
    "#     y= labels[ np.argsort(calculateDistance(test_point,training_features))[:k]]\n",
    "#     l,h = np.unique(y, return_counts=True)\n",
    "#     return l[np.argmax(h)]\n",
    "\n",
    "#     distarr=[]\n",
    "#     for i in range (training_features.shape[0]):\n",
    "#         f=calculateDistance(training_features[i,:],test_point)\n",
    "#         distarr.append(f)\n",
    "        \n",
    "        \n",
    "#     sortedarr=np.sort(distarr)\n",
    "    \n",
    "#     classes=np.zeros(3)\n",
    "    \n",
    "#     for i in range(k):\n",
    "#         result = np.where(distarr == sortedarr[i])\n",
    "#         print(result)\n",
    "#         for j in range(3):\n",
    "#             if(labels[result[0]]==j+1):\n",
    "#                 classes[j]+=1\n",
    "            \n",
    "            \n",
    "#     classification=np.argmax(classes)\n",
    "\n",
    "  \n",
    "#     return classification+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(test_point, training_features, y_train, k):\n",
    "    class1=0\n",
    "    class2=0\n",
    "    class3=0\n",
    "    dist=[]\n",
    "    indexs=[]\n",
    "    \n",
    "    for i in range(training_features.shape[0]):\n",
    "        dist.append(calculateDistance(test_point,training_features[i]))\n",
    "        \n",
    "    dist2=np.argsort(dist)\n",
    "\n",
    "    for i in range(k):\n",
    "        if(y_train[dist2[i]]==1):\n",
    "            class1=class1+1\n",
    "        elif(y_train[dist2[i]]==2):\n",
    "            class2=class2+1\n",
    "        else:\n",
    "            class3=class3+1\n",
    "\n",
    "    if(max(class1,class2,class3)==class1):\n",
    "        classification=1\n",
    "    elif(max(class1,class2,class3)==class2):\n",
    "        classification=2\n",
    "    else:\n",
    "        classification=3\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(training_features, y_train):\n",
    "    clf = SVC(kernel='linear', C=4.0)    #linear sigmoid\n",
    "    clf.fit(training_features, y_train)  \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adaboost_classifier(Y_train, X_train, Y_test, X_test, predictions):\n",
    "\n",
    "#     M = X_train.shape[0]\n",
    "#     w = np.full((M,), (1/M))\n",
    "    \n",
    "#     classValue = []\n",
    "#     alpha_t =np.zeros(len(predictions))\n",
    "        \n",
    "#     for i in range(len(predictions)): \n",
    "\n",
    "#         miss=np.array(1*(predictions[i]==Y_test))\n",
    "\n",
    "#         err_t = (w*(1-miss)).sum()/(w.sum())\n",
    "        \n",
    "#         print((w*(1-miss)).sum())\n",
    "#         print(w.sum())\n",
    "#         print(err_t)\n",
    "\n",
    "#         if(err_t==0):\n",
    "#             alpha_t[i] = (1)\n",
    "#             break\n",
    "#         else if(err_t==1):\n",
    "#             alpha_t[i]=np.min(w)\n",
    "#         else:\n",
    "#             alpha_t[i] = (np.log((1-err_t)/err_t) + np.log(2))\n",
    "\n",
    "#         w=w*np.exp(alpha_t[i]*(1-miss))\n",
    "#         w= w/np.linalg.norm(w)    # Normalize weights\n",
    "        \n",
    "        \n",
    "#     for k in range(3):\n",
    "#         value=0\n",
    "#         for j in range(len(predictions)):\n",
    "#             value+=alpha_t[j]*(predictions[j]==k+1)\n",
    "#         classValue.append(value)\n",
    "        \n",
    "#     print(\"classValue\") \n",
    "#     print(classValue)\n",
    "#     return np.argmax(classValue)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adaboost_classifier(Y_train, training_features,clf,k,ADA_model):     # adaboostof the 3\n",
    "\n",
    "#     M = training_features.shape[0]\n",
    "#     w = np.full((M,), (1/M))\n",
    "    \n",
    "#     classValue = []\n",
    "#     knnPredicts = []\n",
    "#     svmPredicts = []\n",
    "#     adaPredicts = []\n",
    "    \n",
    "    \n",
    "#     #print(training_features[0])\n",
    "#     for i in range(M):\n",
    "#         knnPredicts.append((KNN(training_features[i], training_features, Y_train, k)))\n",
    "#         svmPredicts.append((clf.predict([training_features[i]])[0]))\n",
    "#         adaPredicts.append(ADA_model.predict([training_features[i]])[0])\n",
    "        \n",
    "      \n",
    "#     predictions = [knnPredicts,svmPredicts,adaPredicts]\n",
    "#     alpha_t =np.zeros(len(predictions))\n",
    "\n",
    "#     for i in range(len(predictions)):  \n",
    "        \n",
    "#         miss=np.array(1*(predictions[i]==Y_train))\n",
    "      \n",
    "\n",
    "#         err_t = (w*(1-miss)).sum()/(w.sum())\n",
    "        \n",
    "\n",
    "\n",
    "#         if(err_t==0):\n",
    "#             alpha_t[i] = (1)\n",
    "#             break\n",
    "#         elif(err_t==1):\n",
    "#             alpha_t[i]=np.min(w)\n",
    "#         else:\n",
    "#             alpha_t[i] = (np.log((1-err_t)/err_t) + np.log(2))\n",
    "\n",
    "#         w=w*np.exp(alpha_t[i]*(1-miss))\n",
    "#         w= w/np.linalg.norm(w)    # Normalize weights\n",
    "\n",
    "        \n",
    "#     return alpha_t  \n",
    "\n",
    "def adaboost_classifier(Y_train, training_features,clf,ADA_model):     # adaboost of svm and ada\n",
    "\n",
    "    M = training_features.shape[0]\n",
    "    w = np.full((M,), (1/M))\n",
    "    \n",
    "    classValue = []\n",
    "    svmPredicts = []\n",
    "    adaPredicts = []\n",
    "    \n",
    "    \n",
    "    #print(training_features[0])\n",
    "    for i in range(M):\n",
    "        svmPredicts.append((clf.predict([training_features[i]])[0]))\n",
    "        adaPredicts.append(ADA_model.predict([training_features[i]])[0])\n",
    "        \n",
    "      \n",
    "    predictions = [svmPredicts,adaPredicts]\n",
    "    alpha_t =np.zeros(len(predictions))\n",
    "\n",
    "    for i in range(len(predictions)):  \n",
    "        \n",
    "        miss=np.array(1*(predictions[i]==Y_train))\n",
    "      \n",
    "\n",
    "        err_t = (w*(1-miss)).sum()/(w.sum())\n",
    "        \n",
    "\n",
    "\n",
    "        if(err_t==0):\n",
    "            alpha_t[i] = (1)\n",
    "            break\n",
    "        elif(err_t==1):\n",
    "            alpha_t[i]=np.min(w)\n",
    "        else:\n",
    "            alpha_t[i] = (np.log((1-err_t)/err_t) + np.log(2))\n",
    "\n",
    "        w=w*np.exp(alpha_t[i]*(1-miss))\n",
    "        w= w/np.linalg.norm(w)    # Normalize weights\n",
    "\n",
    "        \n",
    "    return alpha_t \n",
    "        \n",
    "def adaboost_predict (X_test, alpha_t, predictions): \n",
    "    classValue = []\n",
    "    for k in range(3):\n",
    "        value=0\n",
    "        for j in range(len(predictions)):\n",
    "            value+=alpha_t[j]*(predictions[j]==k+1)\n",
    "        classValue.append(value)\n",
    "        \n",
    "\n",
    "    return np.argmax(classValue)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(directory,true_values,l):\n",
    "    training_data,trainTime = train(directory)\n",
    "\n",
    "    training_data = np.asarray(training_data)\n",
    "\n",
    "    labels = training_data[:,0]\n",
    "    training_features = training_data[:,1:]\n",
    "    \n",
    "    test_images = sorted(glob.glob(directory+'/*.png'))\n",
    "    \n",
    "#     true_values.append(int(str(os.path.basename(test_images[0])).split(\".\")[0]))\n",
    "#     f3 = open(directory+\"/ids.txt\", \"r\")\n",
    "    \n",
    "    f3 = open(directory+'/ids.txt')\n",
    "    ids = f3.readlines()\n",
    "    ids=np.asarray(ids,dtype=int)\n",
    "    xtest=ids[-1]\n",
    "    ytest=np.where(ids==xtest)[0][0]+1 \n",
    "    true_values.append(ytest)\n",
    "    f3.close()\n",
    "    \n",
    "    k = 1\n",
    "    knn_prediction = []\n",
    "    SVM_prediction=[]\n",
    "    ADA_prediction=[]\n",
    "    ADA2_prediction=[]\n",
    "    \n",
    "    totalTime=[]\n",
    "\n",
    "    f = open(\"time.txt\", \"a\")\n",
    "    f2= open(\"results.txt\",\"a\")\n",
    "    \n",
    "    clf= SVM(training_features,labels)\n",
    "    \n",
    "    ada = AdaBoostClassifier(n_estimators=50, learning_rate=0.25)    #ADA\n",
    "    ADA_model = ada.fit(training_features, labels)\n",
    "    \n",
    "    alpha_t = adaboost_classifier(labels, training_features,clf, ADA_model)    # ADA2\n",
    "    \n",
    "   \n",
    "    for i in range(len(test_images)):\n",
    "    \n",
    "        img_original = cv2.imread(test_images[i],0)\n",
    "        start = time.time()\n",
    "        greyscale1, binarized1, segmentsBinarized1, segmentsGrey1 = Preprocess(img_original)\n",
    "        maxOccurSegment=[]\n",
    "        maxOccurSegmentSVM=[]\n",
    "        maxOccurSegmentADA=[]\n",
    "        maxOccurSegmentADA2=[]\n",
    "        print(\"Actual class :\", true_values[l], \" of file \", test_images[i])\n",
    "        print(\"---------------------------------------\")\n",
    "        for j in range(segmentsGrey1.shape[0]):\n",
    "            test_point = extract_features(greyscale1, binarized1, segmentsBinarized1, segmentsGrey1[j])\n",
    "\n",
    "            maxOccurSegment.append(int(KNN(test_point, training_features, labels, k)))\n",
    "            maxOccurSegmentSVM.append(int(clf.predict([test_point])[0]))  #SVM\n",
    "            maxOccurSegmentADA.append(int(ADA_model.predict([test_point])[0]))  #ADA\n",
    "            \n",
    "            predictions = [maxOccurSegmentSVM[j], maxOccurSegmentADA[j]]\n",
    "#             maxOccurSegmentADA2.append(adaboost_classifier(labels, training_features, true_values[l], test_point, predictions)) #ADA2 #\n",
    "            maxOccurSegmentADA2.append(adaboost_predict(test_point, alpha_t, predictions)) #ADA2 #\n",
    "\n",
    "\n",
    "        maxOccurSegment=np.asarray(maxOccurSegment)\n",
    "        maxOccurSegmentSVM=np.asarray(maxOccurSegmentSVM)\n",
    "        maxOccurSegmentADA=np.asarray(maxOccurSegmentADA)\n",
    "        maxOccurSegmentADA2=np.asarray(maxOccurSegmentADA2)\n",
    "\n",
    "        print(maxOccurSegment)\n",
    "        print(maxOccurSegmentSVM)\n",
    "        print(maxOccurSegmentADA)\n",
    "        print(maxOccurSegmentADA2)\n",
    "        \n",
    "        knn_prediction.append(np.bincount(maxOccurSegment).argmax())\n",
    "        SVM_prediction.append(np.bincount(maxOccurSegmentSVM).argmax())\n",
    "        ADA_prediction.append(np.bincount(maxOccurSegmentADA).argmax())\n",
    "        ADA2_prediction.append(np.bincount(maxOccurSegmentADA2).argmax())\n",
    "        \n",
    "        end = time.time()\n",
    "        predictionTime= end-start\n",
    "        totalTime.append(trainTime+ predictionTime)\n",
    "\n",
    "        f.write(str(round(totalTime[i],2)))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f2.write(str(knn_prediction[i]))\n",
    "        f2.write(\"\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"knn_prediction class :\", knn_prediction[i])\n",
    "        print(\"SVM_prediction class :\", SVM_prediction[i])\n",
    "        print(\"ADA_prediction class :\", ADA_prediction[i])\n",
    "        print(\"ADA2_prediction class :\", ADA2_prediction[i])\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#     f.write(\"------------------------------------\")\n",
    "#     totalTime=np.asarray(totalTime)\n",
    "#     f.write(str(np.mean(totalTime)))\n",
    "    f.close()\n",
    "    f2.close()\n",
    "\n",
    "    \n",
    "    return knn_prediction, SVM_prediction, ADA_prediction, ADA2_prediction, totalTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------- data\\100\\1\\b06-045.png ------------------------------\n",
      "----------------------- data\\100\\1\\b06-082.png ------------------------------\n",
      "----------------------- data\\100\\2\\k07-077.png ------------------------------\n",
      "----------------------- data\\100\\2\\k07-085.png ------------------------------\n",
      "----------------------- data\\100\\3\\d06-015.png ------------------------------\n",
      "----------------------- data\\100\\3\\d06-082.png ------------------------------\n",
      "Actual class : 2  of file  data\\100\\k07-067a.png\n",
      "---------------------------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (9,) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-7bf1005fa1ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mknn_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVM_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADA_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADA2_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeTest\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mknn_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mSVM_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVM_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-c4fd6db3db61>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(directory, true_values, l)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mtest_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreyscale1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarized1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentsBinarized1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentsGrey1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mmaxOccurSegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mmaxOccurSegmentSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mmaxOccurSegmentADA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mADA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#ADA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-350add55b29f>\u001b[0m in \u001b[0;36mKNN\u001b[1;34m(test_point, training_features, y_train, k)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculateDistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_point\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdist2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-8a7dfce14cca>\u001b[0m in \u001b[0;36mcalculateDistance\u001b[1;34m(x1, x2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculateDistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (9,) "
     ]
    }
   ],
   "source": [
    "direcs = glob.glob (\"data/*\")\n",
    "true_values = []\n",
    "knn_prediction = []\n",
    "SVM_prediction=[]\n",
    "ADA_prediction=[]\n",
    "ADA2_prediction=[]\n",
    "totalTime=[]\n",
    "\n",
    "for l in range(len(direcs)):\n",
    "    knn_predict, SVM_predict, ADA_predict, ADA2_predict, timeTest  = main(direcs[l],true_values,l)\n",
    "    knn_prediction.append(knn_predict[0])\n",
    "    SVM_prediction.append(SVM_predict[0])\n",
    "    ADA_prediction.append(ADA_predict[0])\n",
    "    ADA2_prediction.append(ADA2_predict[0])\n",
    "    totalTime.append(timeTest[0])\n",
    " \n",
    "f4 = open(\"time.txt\", \"a\")\n",
    "totalTime=np.asarray(totalTime)\n",
    "f4.write(str(np.mean(totalTime)))   \n",
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3, 2, 1, 1, 3, 2, 1, 2, 1, 3, 3, 1, 1, 3, 2, 2, 3, 2, 3, 3, 1, 3, 1, 2, 2, 3, 3, 3, 2, 1, 2, 1, 3, 1, 2, 3, 3, 3, 1, 1, 2, 3, 3, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 1, 2, 2, 1, 3, 3, 3, 2, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 1, 1, 3, 2, 3, 1, 1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 2, 1, 3, 3, 1]\n[3, 2, 1, 1, 3, 2, 1, 2, 1, 3, 3, 1, 1, 3, 2, 2, 1, 2, 3, 3, 1, 3, 1, 2, 2, 3, 3, 3, 2, 3, 2, 1, 3, 1, 2, 3, 3, 3, 1, 1, 2, 2, 3, 3, 1, 2, 2, 2, 1, 2, 1, 3, 3, 3, 2, 2, 1, 3, 3, 3, 2, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 3, 1, 3, 3, 3, 2, 3, 1, 1, 3, 2, 1, 1, 1, 2, 3, 1, 3, 1, 1, 3, 2, 1, 2, 3, 1, 3, 3, 1]\n[3, 2, 1, 1, 3, 2, 1, 2, 1, 3, 3, 1, 1, 3, 2, 2, 3, 2, 3, 3, 1, 3, 1, 1, 2, 3, 3, 3, 2, 1, 2, 1, 3, 1, 2, 3, 3, 3, 1, 1, 2, 2, 3, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 1, 2, 2, 1, 3, 3, 3, 2, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 1, 1, 3, 2, 3, 1, 1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 2, 1, 3, 3, 1]\n[3, 2, 2, 1, 2, 2, 1, 2, 1, 3, 3, 1, 2, 3, 2, 2, 3, 1, 3, 3, 1, 3, 1, 1, 2, 2, 3, 3, 2, 1, 2, 1, 3, 1, 2, 1, 3, 3, 1, 2, 2, 2, 3, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 1, 2, 2, 1, 3, 3, 3, 2, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 3, 1, 3, 3, 3, 2, 3, 1, 1, 3, 2, 1, 1, 1, 2, 3, 3, 3, 1, 1, 3, 2, 1, 2, 3, 1, 3, 3, 1]\n[3, 2, 1, 1, 3, 2, 1, 2, 1, 3, 3, 1, 1, 3, 2, 2, 3, 2, 3, 3, 1, 3, 1, 1, 2, 3, 3, 3, 2, 1, 2, 1, 3, 1, 2, 3, 3, 3, 1, 1, 2, 2, 3, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 1, 2, 2, 1, 3, 3, 3, 2, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 3, 1, 1, 3, 2, 3, 1, 1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 2, 1, 3, 3, 1]\nK-Nearest Neighbour Classifier Accuracy:  91.0 %\nSVM Classifier Accuracy:  98.0 %\nADA Classifier Accuracy:  86.0 %\nADA2 Classifier Accuracy:  98.0 %\n"
     ]
    }
   ],
   "source": [
    "total_predictions =  len(direcs)\n",
    "correct_knn=0\n",
    "correct_SVM=0\n",
    "correct_ADA=0\n",
    "correct_ADA2=0\n",
    "\n",
    "print(true_values)\n",
    "print(knn_prediction)\n",
    "print(SVM_prediction)\n",
    "print(ADA_prediction)\n",
    "print(ADA2_prediction)\n",
    "for i in range(total_predictions):\n",
    "    if(true_values[i]==knn_prediction[i]):\n",
    "        correct_knn=correct_knn+1\n",
    "    if(true_values[i]==SVM_prediction[i]):\n",
    "        correct_SVM=correct_SVM+1\n",
    "    if(true_values[i]==ADA_prediction[i]):\n",
    "        correct_ADA=correct_ADA+1\n",
    "    if(true_values[i]==ADA2_prediction[i]):\n",
    "        correct_ADA2=correct_ADA2+1   \n",
    "\n",
    "\n",
    "accuracy_knn = (correct_knn/total_predictions)*100\n",
    "accuracy_SVM = (correct_SVM/total_predictions)*100\n",
    "accuracy_ADA = (correct_ADA/total_predictions)*100\n",
    "accuracy_ADA2 = (correct_ADA2/total_predictions)*100\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"K-Nearest Neighbour Classifier Accuracy: \", accuracy_knn, \"%\")\n",
    "print(\"SVM Classifier Accuracy: \", accuracy_SVM, \"%\")\n",
    "print(\"ADA Classifier Accuracy: \", accuracy_ADA, \"%\")\n",
    "print(\"ADA2 Classifier Accuracy: \", accuracy_ADA2, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('a01-063.png',0) \n",
    "# greyscale1, binarized1, segmentsBinarized1, segmentsGrey1 = Preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fractal_dimension(img, threshold=0.9):\n",
    "#     Z = 1.0 - img/255\n",
    "#     def boxcount(Z, k):\n",
    "#         S = np.add.reduceat(\n",
    "#             np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "#                                np.arange(0, Z.shape[1], k), axis=1)\n",
    "#         return len(np.where((S > 0) & (S < k*k))[0])\n",
    "#     Z = (Z < threshold)\n",
    "#     p = min(Z.shape)\n",
    "#     n = 2**np.floor(np.log(p)/np.log(2))\n",
    "#     n = int(np.log(n)/np.log(2))\n",
    "#     sizes = 2**np.arange(n, 1, -1)\n",
    "#     counts = []\n",
    "#     for size in sizes:\n",
    "#         counts.append(boxcount(Z, size))\n",
    "#     coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "#     return -coeffs[0]\n",
    "\n",
    "\n",
    "# # Z = 1.0 - misc.imread(\"../data/Great-Britain.png\")/255\n",
    "# img = cv2.imread('1.png',0) \n",
    "# greyscale1, binarized1, segmentsBinarized1, segmentsGrey1 = Preprocess(img)\n",
    "# for i in range(len(segmentsGrey1)):\n",
    "#     print(fractal_dimension(segmentsGrey1[i], threshold=0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}